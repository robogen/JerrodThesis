\documentclass[]{scrartcl}

%opening
\title{Thesis Draft}
\author{Jerrod Dixon}

\begin{document}

\maketitle

\section{Introduction}
A grid network is classically defined as a software solution, implementing general commodity hardware, to enable the creation of a series of machines to achieve High Performance Computing (HPC). The benefit here, is that by implementing such a solution, allows for cheaper access to HPC style computing without requiring investment into designing and implementing specialized hardware to improve scientific computations.

In HPC solutions, following the previous definition, there are times when the cluster is not fully in use by the institution that developed it. Conversely, there are also times when the cluster is needed to be used but is at a maximum capacity for work. Because of this, institutions across the globe have come up with methods to connect their personal clusters to a international grid of geographically distant locations, sharing work between each other. In this way, these HPC solutions are better utilized to achieve a higher usage saturation point, taking work from excessively busy systems and giving work to idle ones.

CERN laboratories have used this model of computation to implement their own international grid network (referred to as the CMS project). Via this grid, data produced by experiments performed at CERN with the Large Hadron Collider (LHC) can be sent offsite not only for storage of the data but for processing as well. The type of data produced in these experiments are of such an architecture, that standard processing methods such as utilizing a single machine are not sufficient to process the data in a reasonable amount of time. By using this HPC style of computing the data can be processed in a significantly shorter duration.

Over the years, the processing methods have improved, and the implementation of the LHC itself has grown in such a way that more data is produced than can be processed in a reasonable amount of time. This has resulted in backlogs of data requiring processing. A part of this issue is in the transference of the data to offsite locations. Depending on which labs require the dataset, the data can be replicated in multiple geographically distant locations in such a way to allow easier access. However, sometimes the data being requested gets called to a processing center that is not geographically close slowing the acquisition time.  

\end{document}
